# Metacontext Developer Guide

## Introduction

This guide helps developers understand and extend the Metacontext system. It covers core concepts, architecture, and practical examples for adding new features.

## Core Components

### Two-Tier Architecture

Metacontext uses a two-tier architecture to separate guaranteed deterministic metadata from AI-enhanced insights:

- **Deterministic Tier**: Facts extracted through code execution
- **AI Enrichment Tier**: Interpretive analysis generated by LLMs or AI companions

Each tier is represented by distinct schema classes, allowing for clear separation and graceful degradation when AI is unavailable.

### Schema System

The schema system is built on Pydantic models with a core + extensions architecture:

- **Core Schemas**: `Metacontext`, `FileInfo`, `GenerationInfo`, etc.
- **Extension Schemas**: `ModelContext`, `DataStructure`, `GeospatialContext`, `SpatialExtension`, etc.

Each extension includes both deterministic and AI enrichment components.

### Handler Composition System

File handlers provide specialized processing for different file types with extension support:

- **BaseFileHandler**: Abstract base class for all handlers
- **Specialized Handlers**: `CSVHandler`, `ModelHandler`, `GeospatialHandler`, `MediaHandler`
- **Extensions**: `GeospatialExtension` for spatial metadata enhancement

The handler registry executes a composition workflow:
1. Select appropriate base handler for file type
2. Identify applicable extensions for enhancement  
3. Combine base handler + extensions for complete analysis

Handlers implement file detection, deterministic analysis, and AI enrichment generation.

### Semantic Analysis System

Metacontext v0.3.0 introduces a comprehensive **semantic codebase analysis system** that extracts business context from codebases to enhance AI understanding. This system processes Python codebases through six phases:

#### Core Components

- **SemanticExtractor**: AST-based parsing for functions, classes, and assignments
- **PydanticSchemaExtractor**: Mining field descriptions and validation rules
- **AdvancedSemanticExtractor**: Pattern recognition for constants, enums, and business logic
- **SemanticKnowledgeGraph**: Relationship building with confidence scoring
- **LLMOptimizedOutputGenerator**: Multi-format output for different AI contexts

#### Usage Example

```python
from metacontext.ai.prompts.context_preprocessor import build_semantic_knowledge_graph

# Analyze a project directory
knowledge_graph = build_semantic_knowledge_graph(files_content)

# Access extracted information
for column_name, knowledge in knowledge_graph.column_knowledge.items():
    print(f"Column: {column_name}")
    print(f"Definition: {knowledge.definition}")
    print(f"Business Context: {knowledge.business_context}")
    print(f"Confidence: {knowledge.confidence}")
```

#### Extending the System

To add new semantic extraction patterns:

1. **Enhance Pattern Recognition**: Add new regex patterns to `BUSINESS_LOGIC_PATTERNS` or `CONSTANT_DEFINITION_PATTERNS`
2. **Extend AST Analysis**: Override visitor methods in `AdvancedSemanticExtractor`
3. **Add Output Formats**: Create new format generators in `LLMOptimizedOutputGenerator`
4. **Improve Confidence Scoring**: Modify the confidence calculation algorithm in `SemanticKnowledgeGraph`

### AI Companion Integration

The companion system enables integration with IDE-based AI tools like GitHub Copilot:

#### Core Components

- **BaseCompanionProvider**: Abstract interface for companion providers
- **GitHubCopilotProvider**: Specific implementation for GitHub Copilot
- **TemplateAdapter**: Converts API templates for companion consumption
- **CompanionProviderFactory**: Auto-detection and provider selection

#### Key Features

- **Unified Architecture**: Same workflow as API mode with different execution
- **Template Adaptation**: Automatic conversion of API prompts for IDE consumption
- **Multi-platform Support**: Works across macOS, Windows, and Linux
- **Graceful Fallback**: Falls back to API mode if companions unavailable

## Working with Schemas

### Schema Field Descriptions

Schema fields should include descriptions using the Pydantic `Field` constructor:

```python
from pydantic import BaseModel, Field

class ModelAIEnrichment(AIEnrichment):
    """AI-generated insights about ML models."""
    
    model_type_analysis: str = Field(
        default=None,
        description="Detailed assessment of the model architecture and type, including framework, algorithm family, and key architectural elements.",
    )
```

These descriptions serve two purposes:
1. Documentation for developers
2. Guidance for LLMs in schema-based prompts

### Adding New Schema Extensions

To add a new schema extension:

1. Create a new file in `src/metacontext/schemas/extensions/`
2. Define deterministic and AI enrichment schemas
3. Add field descriptions to all AI enrichment fields
4. Register the extension in the schema registry

## Working with Prompts

### Schema-Based Prompt Generation

Metacontext supports generating prompts directly from schema definitions:

#### Option 1: Template-Based Approach

Create a YAML template that references a schema class:

```yaml
system: |
  You are a data analysis expert tasked with interpreting dataset columns.

instruction: |
  Analyze this dataset column:
  
  Column Name: ${column_name}
  Data Type: ${data_type}
  Sample Values: ${sample_values}
  
  Provide a detailed analysis with these specific fields:
  ${field_descriptions}

schema_class: metacontext.schemas.extensions.tabular.ColumnAIEnrichment
```

Load and render the template:

```python
prompt = prompt_loader.render_prompt(
    "templates/tabular/column_analysis",
    {
        "column_name": "species_name",
        "data_type": "string",
        "sample_values": ["sparrow", "robin", "eagle"]
    }
)
```

#### Option 2: Direct Schema-Based Approach

Generate prompts directly from schema classes:

```python
prompt = prompt_loader.load_schema_prompt(
    "metacontext.schemas.extensions.tabular.ColumnAIEnrichment",
    system_message="You are a data analysis expert...",
    instruction_template="Analyze this column: ${column_name}\n${field_descriptions}",
    column_name="species_name",
    data_type="string"
)
```

### Creating New Prompt Templates

To create a new prompt template:

1. Create a YAML file in `src/metacontext/ai/prompts/templates/`
2. Include system and instruction sections
3. Reference a schema class using `schema_class` property
4. Use placeholders like `${field_descriptions}` for dynamic content

## Implementing a New Handler

To add support for a new file type:

1. Create a new handler class in `src/metacontext/handlers/`
2. Extend `BaseFileHandler`
3. Implement required methods:
   - `can_handle()`: Detect if a file can be processed
   - `generate_context()`: Generate file-specific context
   - `fast_probe()`: Quick file compatibility check
   - `analyze_deterministic()`: Extract facts from the file
   - `analyze_deep()`: Generate AI insights using schema-based prompts
4. Register the handler in the handler registry using `HandlerRegistry.register()`

### Handler Composition Support

New handlers can leverage the composition system:

```python
class MyHandler(BaseFileHandler):
    def generate_context(self, file_path, data_object=None, ai_companion=None):
        # Use composition workflow for enhanced analysis
        return HandlerRegistry.execute_composition_workflow(
            file_path=file_path,
            base_handler=self,
            data_object=data_object,
            ai_companion=ai_companion,
        )
```

## Implementing a New Extension

To add support for enhancing existing handlers:

1. Create a new extension class in `src/metacontext/extensions/`
2. Implement the extension protocol:
   - `can_extend()`: Check if extension applies to a file/data object
   - `extract_spatial_metadata()` or equivalent: Perform extension analysis
   - Define `result_key`: Key for placing results in output
3. Register the extension in the registry system

### Extension Example

```python
class MyExtension:
    result_key = "my_extension"
    
    def can_extend(self, file_path: Path, data_object: object = None) -> bool:
        """Check if this extension can enhance the analysis."""
        return file_path.suffix.lower() in [".myext", ".special"]
    
    def extract_metadata(self, file_path: Path, data_object: object = None) -> dict:
        """Extract extension-specific metadata."""
        return {"enhanced_data": "extracted_value"}
```

## Testing

### Schema Testing

Test schemas with a variety of input data to ensure validation works as expected:

```python
def test_model_ai_enrichment_schema():
    # Test with valid data
    valid_data = {
        "model_type_analysis": "Random Forest classifier...",
        "purpose": "Bird species classification..."
    }
    
    enrichment = ModelAIEnrichment(**valid_data)
    assert enrichment.model_type_analysis == valid_data["model_type_analysis"]
```

### Prompt Testing

Test schema-based prompt generation to ensure prompts include all necessary information:

```python
def test_schema_based_prompt():
    prompt = prompt_loader.load_schema_prompt(
        "metacontext.schemas.extensions.models.ModelAIEnrichment",
        system_message="You are a machine learning expert...",
        model_type="RandomForest"
    )
    
    assert "machine learning expert" in prompt
    assert "RandomForest" in prompt
    assert "model_type_analysis" in prompt
```

## LLM Integration

### Using Multiple LLM Providers

Metacontext supports multiple LLM providers and AI companions:

```python
# Traditional API Providers
from metacontext.ai.handlers.core.provider_manager import ProviderManager

# Auto-select best available provider
llm_handler = ProviderManager.get_best_available_provider(
    preferred_provider="gemini",
    api_key=api_key
)

# AI Companions
from metacontext.ai.handlers.companions import GitHubCopilotProvider

companion = GitHubCopilotProvider()
if companion.is_available():
    # Use companion mode
    output_path = metacontextualize(
        data_object=data,
        file_path=file_path,
        ai_companion=companion
    )
```

### Unified Architecture

Both API and companion modes follow the same workflow:

1. **File Detection & Handler Selection**
2. **Deterministic Analysis** (Tier 1) 
3. **Context Preprocessing** (including semantic analysis)
4. **AI Enrichment** (Tier 2) - Mode-specific execution
5. **Output Generation & Validation**

### Template System

Create templates that work for both modes:

```yaml
# templates/my_analysis.yaml
system: |
  You are an expert data analyst...

instruction: |
  Analyze this file: ${file_name}
  ${semantic_knowledge}
  
  Provide analysis in this structure:
  ${schema_hint}

schema_class: metacontext.schemas.extensions.my_schema.MyAIEnrichment
```

The system automatically adapts templates for companion mode by:
- Converting JSON schema to YAML output format
- Removing API-specific constraints
- Adding workspace context instructions

### Handling LLM Errors

Use the error handling system to gracefully degrade when LLMs are unavailable:

```python
try:
    ai_enrichment = llm_handler.generate_with_schema(
        ModelAIEnrichment,
        context_data
    )
except (LLMError, ValidationRetryError):
    logger.warning("AI enrichment failed, using fallback values")
    ai_enrichment = ModelAIEnrichment(
        purpose="Purpose information unavailable - AI analysis failed"
    )
```

## Best Practices

1. **Schema Design**: Create clear, well-documented schemas with detailed field descriptions
2. **Error Handling**: Implement graceful degradation for all AI-dependent components
3. **Testing**: Test with both available and unavailable AI to ensure robustness
4. **Documentation**: Keep documentation up-to-date with schema and prompt changes
5. **Performance**: Use bulk analysis and schema-based prompts for efficiency
