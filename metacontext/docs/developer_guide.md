# Metacontext Developer Guide

## Introduction

This guide helps developers understand and extend the Metacontext system. It covers core concepts, architecture, and practical examples for adding new features.

## Core Components

### Two-Tier Architecture

Metacontext uses a two-tier architecture to separate guaranteed deterministic metadata from AI-enhanced insights:

- **Deterministic Tier**: Facts extracted through code execution
- **AI Enrichment Tier**: Interpretive analysis generated by LLMs

Each tier is represented by distinct schema classes, allowing for clear separation and graceful degradation when AI is unavailable.

### Schema System

The schema system is built on Pydantic models with a core + extensions architecture:

- **Core Schemas**: `Metacontext`, `FileInfo`, `GenerationInfo`, etc.
- **Extension Schemas**: `ModelContext`, `DataStructure`, `GeospatialContext`, etc.

Each extension includes both deterministic and AI enrichment components.

### Handler System

File handlers provide specialized processing for different file types:

- **BaseFileHandler**: Abstract base class for all handlers
- **Specialized Handlers**: `CSVHandler`, `ModelHandler`, etc.

Handlers implement file detection, deterministic analysis, and AI enrichment generation.

### Semantic Analysis System

Metacontext v0.3.0 introduces a comprehensive **semantic codebase analysis system** that extracts business context from codebases to enhance AI understanding. This system processes Python codebases through six phases:

#### Core Components

- **SemanticExtractor**: AST-based parsing for functions, classes, and assignments
- **PydanticSchemaExtractor**: Mining field descriptions and validation rules
- **AdvancedSemanticExtractor**: Pattern recognition for constants, enums, and business logic
- **SemanticKnowledgeGraph**: Relationship building with confidence scoring
- **LLMOptimizedOutputGenerator**: Multi-format output for different AI contexts

#### Usage Example

```python
from metacontext.ai.prompts.context_preprocessor import build_semantic_knowledge_graph

# Analyze a project directory
knowledge_graph = build_semantic_knowledge_graph("/path/to/project")

# Access extracted information
for column_name, knowledge in knowledge_graph.column_knowledge.items():
    print(f"Column: {column_name}")
    print(f"Definition: {knowledge.definition}")
    print(f"Business Context: {knowledge.business_context}")
    print(f"Confidence: {knowledge.confidence}")
```

#### Extending the System

To add new semantic extraction patterns:

1. **Enhance Pattern Recognition**: Add new regex patterns to `BUSINESS_LOGIC_PATTERNS` or `CONSTANT_DEFINITION_PATTERNS`
2. **Extend AST Analysis**: Override visitor methods in `AdvancedSemanticExtractor`
3. **Add Output Formats**: Create new format generators in `LLMOptimizedOutputGenerator`
4. **Improve Confidence Scoring**: Modify the confidence calculation algorithm in `SemanticKnowledgeGraph`

## Working with Schemas

### Schema Field Descriptions

Schema fields should include descriptions using the Pydantic `Field` constructor:

```python
from pydantic import BaseModel, Field

class ModelAIEnrichment(AIEnrichment):
    """AI-generated insights about ML models."""
    
    model_type_analysis: str = Field(
        default=None,
        description="Detailed assessment of the model architecture and type, including framework, algorithm family, and key architectural elements.",
    )
```

These descriptions serve two purposes:
1. Documentation for developers
2. Guidance for LLMs in schema-based prompts

### Adding New Schema Extensions

To add a new schema extension:

1. Create a new file in `src/metacontext/schemas/extensions/`
2. Define deterministic and AI enrichment schemas
3. Add field descriptions to all AI enrichment fields
4. Register the extension in the schema registry

## Working with Prompts

### Schema-Based Prompt Generation

Metacontext supports generating prompts directly from schema definitions:

#### Option 1: Template-Based Approach

Create a YAML template that references a schema class:

```yaml
system: |
  You are a data analysis expert tasked with interpreting dataset columns.

instruction: |
  Analyze this dataset column:
  
  Column Name: ${column_name}
  Data Type: ${data_type}
  Sample Values: ${sample_values}
  
  Provide a detailed analysis with these specific fields:
  ${field_descriptions}

schema_class: metacontext.schemas.extensions.tabular.ColumnAIEnrichment
```

Load and render the template:

```python
prompt = prompt_loader.render_prompt(
    "templates/tabular/column_analysis",
    {
        "column_name": "species_name",
        "data_type": "string",
        "sample_values": ["sparrow", "robin", "eagle"]
    }
)
```

#### Option 2: Direct Schema-Based Approach

Generate prompts directly from schema classes:

```python
prompt = prompt_loader.load_schema_prompt(
    "metacontext.schemas.extensions.tabular.ColumnAIEnrichment",
    system_message="You are a data analysis expert...",
    instruction_template="Analyze this column: ${column_name}\n${field_descriptions}",
    column_name="species_name",
    data_type="string"
)
```

### Creating New Prompt Templates

To create a new prompt template:

1. Create a YAML file in `src/metacontext/ai/prompts/templates/`
2. Include system and instruction sections
3. Reference a schema class using `schema_class` property
4. Use placeholders like `${field_descriptions}` for dynamic content

## Implementing a New Handler

To add support for a new file type:

1. Create a new handler class in `src/metacontext/handlers/`
2. Extend `BaseFileHandler`
3. Implement required methods:
   - `can_handle()`: Detect if a file can be processed
   - `generate_context()`: Generate file-specific context
   - `_generate_deterministic_metadata()`: Extract facts from the file
   - `_generate_ai_enrichment()`: Generate AI insights using schema-based prompts
4. Register the handler in the handler registry

## Testing

### Schema Testing

Test schemas with a variety of input data to ensure validation works as expected:

```python
def test_model_ai_enrichment_schema():
    # Test with valid data
    valid_data = {
        "model_type_analysis": "Random Forest classifier...",
        "purpose": "Bird species classification..."
    }
    
    enrichment = ModelAIEnrichment(**valid_data)
    assert enrichment.model_type_analysis == valid_data["model_type_analysis"]
```

### Prompt Testing

Test schema-based prompt generation to ensure prompts include all necessary information:

```python
def test_schema_based_prompt():
    prompt = prompt_loader.load_schema_prompt(
        "metacontext.schemas.extensions.models.ModelAIEnrichment",
        system_message="You are a machine learning expert...",
        model_type="RandomForest"
    )
    
    assert "machine learning expert" in prompt
    assert "RandomForest" in prompt
    assert "model_type_analysis" in prompt
```

## LLM Integration

### Using Multiple LLM Providers

Metacontext supports multiple LLM providers:

```python
# OpenAI
openai_handler = OpenAIProvider(
    model="gpt-4",
    api_key=os.environ["OPENAI_API_KEY"]
)

# Anthropic
anthropic_handler = AnthropicProvider(
    model="claude-3-opus-20240229",
    api_key=os.environ["ANTHROPIC_API_KEY"]
)
```

### Handling LLM Errors

Use the error handling system to gracefully degrade when LLMs are unavailable:

```python
try:
    ai_enrichment = llm_handler.generate_with_schema(
        ModelAIEnrichment,
        context_data
    )
except (LLMError, ValidationRetryError):
    logger.warning("AI enrichment failed, using fallback values")
    ai_enrichment = ModelAIEnrichment(
        purpose="Purpose information unavailable - AI analysis failed"
    )
```

## Best Practices

1. **Schema Design**: Create clear, well-documented schemas with detailed field descriptions
2. **Error Handling**: Implement graceful degradation for all AI-dependent components
3. **Testing**: Test with both available and unavailable AI to ensure robustness
4. **Documentation**: Keep documentation up-to-date with schema and prompt changes
5. **Performance**: Use bulk analysis and schema-based prompts for efficiency
