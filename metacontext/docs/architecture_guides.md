# Metacontext Architecture Guides

## Two-Tier Architecture

### Overview

Metacontext uses a revolutionary two-tier architecture that separates deterministic facts from AI-generated intelligence:

- **Tier 1: Deterministic Metadata (Always Succeeds)**
  - Facts extracted through direct code execution
  - 100% reliable, always available
  - Never fails, no external dependencies

- **Tier 2: AI Enrichment (Best Effort)**
  - Interpretive insights generated by LLMs or AI companions
  - Rich contextual understanding
  - Gracefully degrades when AI unavailable
  - Supports both API mode and companion mode (GitHub Copilot)

### Benefits

- **Trust Model**: Clear separation between facts and interpretations
- **Production Reliability**: Core functionality never blocked by AI failures
- **Confidence Assessment**: All AI-generated content includes confidence indicators
- **Transparency**: Users know exactly what's measured vs. interpreted

### Example

```yaml
model_context:
  deterministic_metadata:        # Tier 1: Facts (Always Available)
    framework: scikit-learn      # ← Object introspection
    model_type: RandomForestClassifier
    hyperparameters: {...}       # ← Direct parameter extraction
    input_shape: [3]             # ← Measured values
    output_shape: [1]
    
  ai_enrichment:                # Tier 2: Insights (When Available)
    purpose: "Bird species classification"  # ← Interpreted meaning
    training_approach: "The model uses..."  # ← Contextual analysis
    limitations: "Limited to the 5 species in training data"
```

## Universal File Intelligence

### Overview

Metacontext provides a universal file intelligence system that works with multiple file types through a sophisticated handler composition system:

- **CSV/DataFrames**: Column analysis, data profiling, and contextual interpretation
- **ML Models**: Model understanding, hyperparameter analysis, and usage guidance
- **Geospatial Data**: Coordinate systems, projection information, and spatial context
- **Media Files**: Content analysis, quality assessment, and metadata extraction

### Handler Composition System

The file handler system automatically detects file types and routes them to appropriate specialized handlers, enhanced by applicable extensions:

```
src/metacontext/handlers/
├── base.py           # BaseFileHandler abstract class
├── tabular.py        # CSV, Excel, DataFrame handler  
├── geospatial.py     # GeoTIFF, Shapefile, GeoJSON handler
├── model.py          # PKL, JobLib, ONNX handler
├── media.py          # Image, Audio, Video handler
└── __init__.py       # Handler registry

src/metacontext/extensions/
├── geospatial.py     # Spatial metadata extension
└── __init__.py       # Extension registry
```

The system uses a composition workflow:
1. **Base Handler Selection**: Primary handler selected based on file type
2. **Extension Discovery**: Applicable extensions identified for enhancement
3. **Composition Execution**: Base handler + extensions combined for complete analysis

### Extension System

Each file type has dedicated schema extensions with both deterministic and AI components:

```yaml
# Core (always present)
metacontext_version: 0.3.0
file_info: {...}
  
# Extensions (conditionally added based on file analysis)
data_structure: {...}      # For tabular data
geospatial_context: {...}  # For spatial data (via extension)
model_context: {...}       # For ML models
spatial_extension: {...}   # For geospatial enhancements
```

## Semantic Codebase Analysis System

### Overview

Metacontext v0.3.0 introduces a revolutionary **semantic codebase analysis system** that extracts business context from codebases to enhance AI understanding. This system addresses the critical problem where LLMs miss embedded business context in code comments, docstrings, and Pydantic schemas.

### Six-Phase Architecture

The semantic analysis system processes codebases through six comprehensive phases:

#### Phase 1: Enhanced Comment & Docstring Extraction
- **Multi-line comment detection** for data dictionaries and business rules
- **AST-based parsing** for function docstrings and class documentation
- **Proximity-based linking** connects comments to nearby code within N lines

#### Phase 2: Pydantic Schema Mining
- **Field description extraction** from `Field(description="...")` annotations
- **Model relationship mapping** for inheritance and composition patterns
- **Validation rule documentation** including `@validator` decorators and constraints

#### Phase 3: Advanced Semantic Extraction
- **Enum and constant analysis** with semantic meaning inference
- **Function analysis** tracking data transformations and column creation
- **Business logic pattern recognition** for conditional logic and scoring algorithms

#### Phase 4: File Type Filtering & Optimization
- **Smart file filtering** with 11 categories and relevance checking
- **Performance optimization** with AST caching and parallel processing
- **Progress tracking** for large codebase analysis

#### Phase 5: Semantic Knowledge Graph Construction
- **Column-context relationships** with confidence scoring
- **Cross-reference resolution** linking names across files
- **Conflict resolution** handling contradictory information gracefully

#### Phase 6: LLM-Optimized Output Generation
- **Six specialized formats**: Context injection, documentation, API specs, prompt templates, structured summaries, debugging guides
- **Context-aware generation** optimized for different AI consumption patterns
- **Token efficiency** with format-specific optimization

### Implementation

```python
from metacontext.ai.prompts.context_preprocessor import build_semantic_knowledge_graph

# Analyze a project directory
knowledge_graph = build_semantic_knowledge_graph(files_content)

# Access extracted information
for column_name, knowledge in knowledge_graph.column_knowledge.items():
    print(f"Column: {column_name}")
    print(f"Definition: {knowledge.definition}")
    print(f"Business Context: {knowledge.business_context}")
    print(f"Confidence: {knowledge.confidence}")
```

### Real-World Impact

The semantic analysis system dramatically improves AI understanding by:
- **Extracting semantic relationships** from code comments and documentation
- **Building confidence-scored knowledge graphs** connecting data columns to business logic
- **Consolidating contradictory information** from multiple sources
- **Providing LLM-optimized output formats** for different AI consumption patterns

## AI Companion Integration

### Overview

Metacontext v0.3.0 introduces **AI companion integration** that leverages IDE-integrated tools like GitHub Copilot for enhanced analysis while maintaining the same two-tier architecture.

### Companion Provider System

```
src/metacontext/ai/handlers/companions/
├── companion_provider.py    # Base companion interface
├── copilot_provider.py     # GitHub Copilot implementation
├── template_adapter.py     # Template conversion system
├── ide_automation.py       # IDE integration utilities
└── companion_factory.py    # Provider factory and detection
```

### Unified Architecture

Both API mode and companion mode follow the identical workflow:

1. **File Detection & Handler Selection**
2. **Deterministic Analysis** (Tier 1)
3. **Context Preprocessing** (including semantic analysis)
4. **AI Enrichment** (Tier 2) - **Mode-specific execution**:
   - **API Mode**: Direct LLM API calls with JSON responses
   - **Companion Mode**: Clipboard workflow with YAML responses
5. **Output Generation & Validation**

### Template Adaptation

The companion system automatically adapts existing API templates for IDE consumption:
- **Removes JSON schema constraints** and API-specific formatting
- **Converts to YAML output format** for human-readable responses
- **Preserves core analysis instructions** and forensic investigation approach
- **Adds workspace-aware analysis** leveraging IDE context

### Usage

```python
# Automatic companion detection
output_path = metacontextualize(
    data_object=df,
    file_path="data.csv",
    llm_provider="companion"  # Auto-detects GitHub Copilot
)

# Force companion mode
from metacontext.ai.handlers.companions import GitHubCopilotProvider
companion = GitHubCopilotProvider()

output_path = metacontextualize(
    data_object=df,
    file_path="data.csv",
    ai_companion=companion
)
```