# Metacontext Architecture Guides

## Two-Tier Architecture

### Overview

Metacontext uses a revolutionary two-tier architecture that separates deterministic facts from AI-generated intelligence:

- **Tier 1: Deterministic Metadata (Always Succeeds)**
  - Facts extracted through direct code execution
  - 100% reliable, always available
  - Never fails, no external dependencies

- **Tier 2: AI Enrichment (Best Effort)**
  - Interpretive insights generated by LLMs
  - Rich contextual understanding
  - Gracefully degrades when AI unavailable

### Benefits

- **Trust Model**: Clear separation between facts and interpretations
- **Production Reliability**: Core functionality never blocked by AI failures
- **Confidence Assessment**: All AI-generated content includes confidence indicators
- **Transparency**: Users know exactly what's measured vs. interpreted

### Example

```yaml
model_context:
  deterministic_metadata:        # Tier 1: Facts (Always Available)
    framework: scikit-learn      # ← Object introspection
    model_type: RandomForestClassifier
    hyperparameters: {...}       # ← Direct parameter extraction
    input_shape: [3]             # ← Measured values
    output_shape: [1]
    
  ai_enrichment:                # Tier 2: Insights (When Available)
    purpose: "Bird species classification"  # ← Interpreted meaning
    training_approach: "The model uses..."  # ← Contextual analysis
    limitations: "Limited to the 5 species in training data"
```

## Universal File Intelligence

### Overview

Metacontext provides a universal file intelligence system that works with multiple file types:

- **CSV/DataFrames**: Column analysis, data profiling, and contextual interpretation
- **ML Models**: Model understanding, hyperparameter analysis, and usage guidance
- **Geospatial Data**: Coordinate systems, projection information, and spatial context
- **Media Files**: Content analysis, quality assessment, and metadata extraction

### Handler System

The file handler system automatically detects file types and routes them to the appropriate specialized handler:

```
src/metacontext/handlers/
├── base.py           # BaseFileHandler abstract class
├── tabular.py        # CSV, Excel, DataFrame handler  
├── geospatial.py     # GeoTIFF, Shapefile, GeoJSON handler
├── model.py          # PKL, JobLib, ONNX handler
├── media.py          # Image, Audio, Video handler
└── __init__.py       # Handler registry
```

### Extension System

Each file type has dedicated schema extensions with both deterministic and AI components:

```yaml
# Core (always present)
metacontext_version: 0.3.0
file_info: {...}
  
# Extensions (conditionally added)
data_structure: {...}      # For tabular data
geospatial_context: {...}  # For spatial data  
model_context: {...}       # For ML models
```

## Schema-Prompt System

### Overview

The schema-prompt system provides a structured approach to generating LLM prompts and validating responses:

- **Schema-Based Prompts**: Generated from Pydantic models
- **Bulk Contextual Analysis**: Efficient prompts that analyze multiple fields at once
- **Validation**: Automatic validation of LLM responses against schemas
- **Error Handling**: Retry logic and fallbacks for invalid responses

### From Field-by-Field to Bulk Intelligence

**Old Approach (Inefficient):**
1. Map each field to individual prompt
2. Generate 20+ separate API calls per file
3. Limited to tabular data formats
4. Static, rigid schema structure

**New Approach (Efficient):**
1. **File type detection** → route to appropriate handler
2. **Bulk contextual prompts** → 3-5 API calls per file  
3. **Universal file support** → any format with appropriate extensions
4. **Modular schema** → core + file-type-specific extensions

### Schema-Based Prompt Generation

Metacontext supports schema-based prompt generation, which simplifies prompt maintenance by directly generating prompts from Pydantic schema definitions. See [Schema-Based Prompts](./schema_based_prompts.md) for details.

## Codebase Scanning Feature

### Overview

The codebase scanning feature analyzes the project codebase to provide rich context for metadata generation:

- **Project Understanding**: Scan code, documentation, and structure
- **Contextual Analysis**: Use code patterns to understand data purpose
- **Relationship Discovery**: Find connections between data files and code
- **Enhanced Prompts**: AI gets full project context, not just data

### Implementation

```python
from metacontext import CodebaseScanner

# Create scanner from current working directory
scanner = CodebaseScanner()

# Scan for context related to a specific file
context = scanner.scan_for_file_context("data/birds.csv")

# Generate enhanced metadata with codebase context
metadata = metacontextualize("data/birds.csv", codebase_context=context)
```

### Benefits

- **Deeper Understanding**: LLM sees how data is used in the project
- **Better Metadata**: More accurate analysis based on actual usage
- **Less User Input**: System automatically discovers context
- **Code-Aware**: Understands data types, transformations, and purpose