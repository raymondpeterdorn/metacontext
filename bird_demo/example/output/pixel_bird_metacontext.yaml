metacontext_version: 0.3.0
generation_info:
  generated_at: '2025-09-17T23:14:10.341081+00:00'
  generation_method: fallback
  function_call: metacontext.metacontextualize()
file_info:
  filename: pixel_bird.png
  extension: .png
  absolute_path: /Users/bigdawg/Documents/repos/metacontext/bird_demo/example/output/pixel_bird.png
  directory: example/output
system_info:
  working_directory: /Users/bigdawg/Documents/repos/metacontext/bird_demo
data_analysis:
  type: generic_object
  python_type: Image
  note: Fallback analysis - specialized handler not available
confidence_assessment:
  overall: LOW
codebase_context:
  project_info:
    project_root: /Users/bigdawg/Documents/repos/metacontext/bird_demo
    readme_files:
    - path: README.md
      size_bytes: 2215
      preview: 'Bird Anatomy and Species Classification Project

        This repository contains an example codebase for data analysis and machine
        learning. The primary goal of this project is to explore the relationships
        between various anatomical measurements of birds and to build a predictive
        model for species classification.


        Business Context & Purpose

        The data was collected by a team of ornithologists to better understand the
        physical characteristics that define different bird species. The key questions
        we aim to answer are:


        What are the key statistical characteristics of the collected data?


        Can we predict a bird''s species based on its physical measurements alone?

        '
    - path: .venv/lib/python3.12/site-packages/langsmith/cli/README.md
      size_bytes: 222
      preview: '# DOCKER-COMPOSE MOVED

        All documentation for docker-compose has been moved to the helm repository.

        You can find it [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docker-compose/docker-compose.yaml)'
    - path: .venv/lib/python3.12/site-packages/numpy/ma/README.rst
      size_bytes: 9874
      preview: '==================================

        A guide to masked arrays in NumPy

        ==================================


        .. Contents::


        See http://www.scipy.org/scipy/numpy/wiki/MaskedArray (dead link)

        for updates of this document.


        '
    - path: .venv/lib/python3.12/site-packages/numpy/_core/tests/data/umath-validation-set-README.txt
      size_bytes: 967
      preview: "Steps to validate transcendental functions:\n1) Add a file 'umath-validation-set-<ufuncname>.txt',\
        \ where ufuncname is name of\n   the function in NumPy you want to validate\n\
        2) The file should contain 4 columns: dtype,input,expected output,ulperror\n\
        \    a. dtype: one of np.float16, np.float32, np.float64\n    b. input: floating\
        \ point input to ufunc in hex. Example: 0x414570a4\n       represents 12.340000152587890625\n\
        \    c. expected output: floating point output for the corresponding input\
        \ in hex.\n       This should be computed using a high(er) precision library\
        \ and then rounded to\n       same format as the input."
    - path: .venv/lib/python3.12/site-packages/sklearn/externals/README
      size_bytes: 270
      preview: 'This directory contains bundled external dependencies that are updated

        every once in a while.


        Note for distribution packagers: if you want to remove the duplicated

        code and depend on a packaged version, we suggest that you simply do a

        symbolic link in this directory.

        '
    - path: .venv/lib/python3.12/site-packages/sklearn/tests/test_min_dependencies_readme.py
      size_bytes: 4576
      preview: '"""Tests for the minimum dependencies in README.rst and pyproject.toml"""


        import os

        import re

        from collections import defaultdict

        from pathlib import Path


        import pytest


        import sklearn'
    - path: .venv/lib/python3.12/site-packages/sklearn/datasets/images/README.txt
      size_bytes: 709
      preview: 'Image: china.jpg

        Released under a creative commons license. [1]

        Attribution: Some rights reserved by danielbuechele [2]

        Retrieved 21st August, 2011 from [3] by Robert Layton


        [1] https://creativecommons.org/licenses/by/2.0/

        [2] https://www.flickr.com/photos/danielbuechele/

        [3] https://www.flickr.com/photos/danielbuechele/6061409035/sizes/z/in/photostream/


        '
    - path: .venv/lib/python3.12/site-packages/sklearn/externals/array_api_compat/README.md
      size_bytes: 67
      preview: Update this directory using maint_tools/vendor_array_api_compat.sh
    - path: .venv/lib/python3.12/site-packages/sklearn/externals/array_api_extra/README.md
      size_bytes: 66
      preview: Update this directory using maint_tools/vendor_array_api_extra.sh
    - path: .venv/lib/python3.12/site-packages/pyogrio/tests/fixtures/README.md
      size_bytes: 3606
      preview: '# Test datasets


        ## Obtaining / creating test datasets


        If a test dataset can be created in code, do that instead. If it is used in
        a

        single test, create the test dataset as part of that test. If it is used in

        more than a single test, add it to `pyogrio/tests/conftest.py` instead, as
        a

        function-scoped test fixture.


        If you need to obtain 3rd party test files:'
    - path: .venv/lib/python3.12/site-packages/pyarrow/tests/data/orc/README.md
      size_bytes: 932
      preview: "<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n\
        \  or more contributor license agreements.  See the NOTICE file\n  distributed\
        \ with this work for additional information\n  regarding copyright ownership.\
        \  The ASF licenses this file\n  to you under the Apache License, Version\
        \ 2.0 (the\n  \"License\"); you may not use this file except in compliance\n\
        \  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0"
    - path: metacontext/ai/prompts/templates/README.md
      size_bytes: 1370
      preview: '# Metacontext Prompt Templates


        This directory contains prompt templates used by the Metacontext system for
        AI analysis.


        ## Directory Structure


        - `general/`: General-purpose templates used across multiple domains

        - `tabular/`: Prompts for analyzing tabular data (CSV, Excel, etc.)

        - `model/`: Prompts for analyzing ML models and their attributes

        - `code/`: Prompts for analyzing code and repository structure'
    changelog_files:
    - path: .venv/lib/python3.12/site-packages/langchain_core/chat_history.py
      size_bytes: 8388
      preview: "\"\"\"**Chat message history** stores a history of the message interactions\
        \ in a chat.\n\n**Class hierarchy:**\n\n.. code-block::\n\n    BaseChatMessageHistory\
        \ --> <name>ChatMessageHistory  # Examples: FileChatMessageHistory, PostgresChatMessageHistory\n\
        \n**Main helpers:**\n"
    - path: .venv/lib/python3.12/site-packages/googleapiclient/discovery_cache/documents/versionhistory.v1.json
      size_bytes: 17987
      preview: '{

        "basePath": "",

        "baseUrl": "https://versionhistory.googleapis.com/",

        "batchPath": "batch",

        "canonicalName": "Version History",

        "description": "Version History API - Prod",

        "discoveryVersion": "v1",

        "documentationLink": "https://developer.chrome.com/docs/web-platform/versionhistory/guide",

        "fullyEncodeReservedExpansion": true,

        "icons": {'
    - path: .venv/lib/python3.12/site-packages/langchain_core/runnables/history.py
      size_bytes: 24775
      preview: "\"\"\"Runnable that manages chat message history for another Runnable.\"\
        \"\"\n\nfrom __future__ import annotations\n\nimport inspect\nfrom collections.abc\
        \ import Sequence\nfrom types import GenericAlias\nfrom typing import (\n\
        \    TYPE_CHECKING,\n    Any,"
    - path: .venv/lib/python3.12/site-packages/numpy/ma/API_CHANGES.txt
      size_bytes: 3405
      preview: '.. -*- rest -*-


        ==================================================

        API changes in the new masked array implementation

        ==================================================


        Masked arrays are subclasses of ndarray

        ---------------------------------------


        Contrary to the original implementation, masked arrays are now regular'
    - path: .venv/lib/python3.12/site-packages/sklearn/svm/src/libsvm/LIBSVM_CHANGES
      size_bytes: 769
      preview: "Changes to Libsvm\n\nThis is here mainly as checklist for incorporation\
        \ of new versions of libsvm.\n\n  * Add copyright to files svm.cpp and svm.h\n\
        \  * Add random_seed support and call to srand in fit function\n  * Improved\
        \ random number generator (fix on windows, enhancement on other\n    platforms).\
        \ See <https://github.com/scikit-learn/scikit-learn/pull/13511#issuecomment-481729756>\n\
        \  * invoke scipy blas api for svm kernel function to improve performance\
        \ with speedup rate of 1.5X to 2X for dense data only. See <https://github.com/scikit-learn/scikit-learn/pull/16530>\n\
        \  * Expose the number of iterations run in optimization. See <https://github.com/scikit-learn/scikit-learn/pull/21408>"
    - path: .venv/lib/python3.12/site-packages/scipy/_lib/pyprima/common/history.py
      size_bytes: 1361
      preview: '''''''

        This module provides subroutines that handle the X/F/C histories of the solver,
        taking into

        account that MAXHIST may be smaller than NF.


        Translated from Zaikun Zhang''s modern-Fortran reference implementation in
        PRIMA.


        Dedicated to late Professor M. J. D. Powell FRS (1936--2015).


        Python translation by Nickolai Belakovski.

        '''''''
    project_config:
    - path: pyproject.toml
      type: pyproject.toml
      content: '[tool.poetry]

        name = "bird_demo"

        version = "0.1.0"

        description = "A project that uses birds to demonstrate how metacontext is
        used"

        authors = ["raymondpeterdorn <rpd346@gmail.com>"]

        readme = "README.md"

        packages = [{include = "bird_demo"}]


        [tool.poetry.dependencies]

        python = ">=3.12,<3.13"

        python-dotenv = ">=1.0.0"

        geopandas = ">=1.1.1,<2.0.0"

        scipy = ">=1.16.2,<2.0.0"

        scikit-learn = ">=1.7.2,<2.0.0"

        numpy = ">=2.3.3,<3.0.0"

        pydantic = ">=2.11.9,<3.0.0"

        pillow = ">=11.3.0,<12.0.0"

        metacontext = {path = "../metacontext", develop = true}


        [tool.poetry.group.dev.dependencies]'
    - path: .venv/lib/python3.12/site-packages/pandas/pyproject.toml
      type: pyproject.toml
      content: "[build-system]\n# Minimum requirements for the build system to execute.\n\
        # See https://github.com/scipy/scipy/pull/12940 for the AIX issue.\nrequires\
        \ = [\n    \"meson-python>=0.13.1\",\n    \"meson>=1.2.1,<2\",\n    \"wheel\"\
        ,\n    \"Cython<4.0.0a0\",  # Note: sync with setup.py, environment.yml and\
        \ asv.conf.json\n    # Force numpy higher than 2.0rc1, so that built wheels\
        \ are compatible\n    # with both numpy 1 and 2\n    \"numpy>=2.0\",\n   \
        \ \"versioneer[toml]\"\n]\n\nbuild-backend = \"mesonpy\"\n\n[project]\nname\
        \ = 'pandas'\ndynamic = [\n  'version'"
    - path: .venv/lib/python3.12/site-packages/numpy/_core/tests/examples/cython/setup.py
      type: setup.py
      content: "\"\"\"\nProvide python-space access to the functions exposed in numpy/__init__.pxd\n\
        for testing.\n\"\"\"\n\nimport os\nfrom distutils.core import setup\n\nimport\
        \ Cython\nfrom Cython.Build import cythonize\nfrom setuptools.extension import\
        \ Extension\n\nimport numpy as np\nfrom numpy._utils import _pep440\n\nmacros\
        \ = [\n    (\"NPY_NO_DEPRECATED_API\", 0),\n    # Require 1.25+ to test datetime\
        \ additions\n    (\"NPY_TARGET_VERSION\", \"NPY_2_0_API_VERSION\"),\n]"
    - path: .venv/lib/python3.12/site-packages/numpy/_core/tests/examples/limited_api/setup.py
      type: setup.py
      content: "\"\"\"\nBuild an example package using the limited Python C API.\n\
        \"\"\"\n\nimport os\n\nfrom setuptools import Extension, setup\n\nimport numpy\
        \ as np\n\nmacros = [(\"NPY_NO_DEPRECATED_API\", 0), (\"Py_LIMITED_API\",\
        \ \"0x03060000\")]\n\nlimited_api = Extension(\n    \"limited_api\",\n   \
        \ sources=[os.path.join('.', \"limited_api.c\")],\n    include_dirs=[np.get_include()],\n\
        \    define_macros=macros,\n)\n\nextensions = [limited_api]"
  related_code:
  - path: bird_demo/demo.py
    relevance_score: 11.0
    file_type: .py
    size_bytes: 11700
    preview: 'import ast

      import logging

      import os

      import pickle

      import sys

      from pathlib import Path

      from typing import Any

      #!/usr/bin/env python3

      """Create a simple bird classification model and save it as pkl for testing."""


      import ast

      import logging

      import os

      import pickle

      import sys'
    has_pandas: true
    has_data_io: true
    has_relevant_functions: []
    imports:
    - ast
    - logging
    - os
    - pickle
    - sys
    - Path
    - Any
    - geopandas as gpd
    - numpy as np
    - pandas as pd
  - path: bird_demo/models.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 707
    preview: "import datetime\nimport zoneinfo\nfrom pydantic import BaseModel, field_validator\n\
      class BirdData(BaseModel):\n    def parse_date(cls, v: str) -> datetime.date:\n\
      # models.py\n\"\"\"Pydantic models for the example.\"\"\"\n\nimport datetime\n\
      import zoneinfo\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass\
      \ BirdData(BaseModel):"
    has_pandas: false
    has_data_io: false
    has_relevant_functions: []
    imports:
    - datetime
    - zoneinfo
    - BaseModel, field_validator
  - path: bird_demo/__init__.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 24
    preview: '"""Init for example."""

      '
    has_pandas: false
    has_data_io: false
    has_relevant_functions: []
    imports: []
  - path: metacontext/architecture_reference.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 6309
    preview: "from dataclasses import dataclass\nclass ArchitectureReference:\n  \
      \  def __str__(self) -> str:\nclass ArchitecturalComponents:\nclass ConfigurationArchitecture:\n\
      class HandlerArchitecture:\nclass LLMArchitecture:\n\"\"\"Architecture reference\
      \ documentation embedded in code.\n\nThis module serves as a reference point\
      \ connecting the code implementation to its\ncorresponding technical documentation.\
      \ It contains no functional code but provides\nannotated documentation references\
      \ that can be used to navigate between code and docs.\n\nThis helps maintain\
      \ the connection between code and documentation, making it easier\nto keep them\
      \ in sync and understand the architectural principles behind the implementation."
    has_pandas: false
    has_data_io: false
    has_relevant_functions: []
    imports:
    - dataclass
  - path: metacontext/metacontextualize.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 18857
    preview: 'import logging

      import time

      import yaml

      from dataclasses import dataclass

      from datetime import UTC, datetime

      from pathlib import Path

      from typing import Any

      """Main API for metacontext generation with file handler routing.


      This module provides the main entry point for metacontextualize() which

      intelligently routes files to appropriate handlers and generates metadata.

      It implements the architectural patterns documented in the architecture reference.


      See:

      - architecture_reference.ArchitecturalComponents.TWO_TIER_ARCHITECTURE'
    has_pandas: false
    has_data_io: false
    has_relevant_functions: []
    imports:
    - logging
    - time
    - yaml
    - dataclass
    - UTC, datetime
    - Path
    - Any
    - scan_codebase_context
    - ProviderManager
    - LLMProvider
  - path: metacontext/__init__.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 737
    preview: "    import metacontext\nfrom metacontext.metacontextualize import MetacontextualizeArgs,\
      \ metacontextualize\n\"\"\"Metacontext: Automatic Metadata Replacement System.\n\
      \nReplaces traditional file metadata with rich, AI-generated context that\n\
      travels with every data file.\n\nSimple Usage:\n    import metacontext\n\n \
      \   # Your existing data export (unchanged)\n    df.to_csv('data.csv')\n\n \
      \   # Generate rich context (one line)\n    metacontext.metacontextualize(df,\
      \ 'data.csv')"
    has_pandas: false
    has_data_io: true
    has_relevant_functions: []
    imports:
    - MetacontextualizeArgs, metacontextualize
  - path: metacontext/cli.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 4373
    preview: "import argparse\nimport sys\nimport traceback\nfrom pathlib import Path\n\
      from .metacontextualize import MetacontextualizeArgs, metacontextualize\ndef\
      \ create_parser() -> argparse.ArgumentParser:\ndef validate_file_path(file_path:\
      \ str) -> Path:\n#!/usr/bin/env python3\n\"\"\"Command Line Interface for Metacontext.\n\
      \nThis module provides a lightweight CLI for generating metacontext from files.\n\
      Users can run metacontext directly from the terminal without writing Python\
      \ code.\n\nExample usage:\n    metacontext data.csv"
    has_pandas: false
    has_data_io: false
    has_relevant_functions: []
    imports:
    - argparse
    - sys
    - traceback
    - Path
    - MetacontextualizeArgs, metacontextualize
  - path: bird_demo/scripts/train_model.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 2876
    preview: 'import logging

      from pathlib import Path

      from typing import Any

      import joblib

      import pandas as pd

      from sklearn.ensemble import RandomForestClassifier

      from sklearn.model_selection import train_test_split

      """Train a model on the bird dataset."""


      import logging

      from pathlib import Path

      from typing import Any


      import joblib

      import pandas as pd'
    has_pandas: true
    has_data_io: true
    has_relevant_functions: []
    imports:
    - logging
    - Path
    - Any
    - joblib
    - pandas as pd
    - RandomForestClassifier
    - train_test_split
    - LabelEncoder
  - path: bird_demo/scripts/__init__.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 23
    preview: '"""Example scripts."""

      '
    has_pandas: false
    has_data_io: false
    has_relevant_functions: []
    imports: []
  - path: bird_demo/scripts/exploratory_data_analysis.py
    relevance_score: 1.0
    file_type: .py
    size_bytes: 7964
    preview: 'import ast

      from datetime import date

      from typing import Any

      import pandas as pd

      from pydantic import BaseModel, Field, ValidationError

      from scipy import stats

      class BirdObservation(BaseModel):

      """data_analysis.py.


      This script performs a full data analysis workflow on bird observation data.

      It includes data loading from a CSV string, validation using a Pydantic model,

      and exploratory data analysis with summary statistics and a t-test.

      """

      import ast

      from datetime import date'
    has_pandas: true
    has_data_io: true
    has_relevant_functions: []
    imports:
    - ast
    - date
    - Any
    - pandas as pd
    - BaseModel, Field, ValidationError
    - stats
  documentation: '[Document(metadata={''path'': ''README.md'', ''type'': ''documentation'',
    ''directory'': ''root'', ''size_bytes'': 2215}, page_content="Bird Anatomy and
    Species Classification Project\nThis repository contains an example codebase for
    data analysis and machine learning. The primary goal of this project is to explore
    the relationships between various anatomical measurements of birds and to build
    a predictive model for species classification.\n\nBusiness Context & Purpose\nThe
    data was collected by a team of ornithologists to better understand the physical
    characteristics that define different bird species. The key questions we aim to
    answer are:\n\nWhat are the key statistical characteristics of the collected data?\n\nCan
    we predict a bird''s species based on its physical measurements alone?\n"), Document(metadata={''path'':
    ''r.txt'', ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'':
    129}, page_content=''ast\nlogging\nos\npickle\nsys\npathlib\ntyping\ngeopandas\nnumpy\npandas''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/google_generativeai-0.8.5.dist-info/namespace_packages.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 7}, page_content=''google''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/google_generativeai-0.8.5.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 7}, page_content=''google''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/pyogrio-0.11.1.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 8}, page_content=''pyogrio''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/requests-2.32.5.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 9}, page_content=''requests''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/idna-3.10.dist-info/LICENSE.md'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 1541}, page_content=''BSD
    3-Clause License\n\nCopyright (c) 2013-2024, Kim Davies and contributors.\nAll
    rights reserved.\n\nRedistribution and use in source and binary forms, with or
    without\nmodification, are permitted provided that the following conditions are\nmet:\n\n1.
    Redistributions of source code must retain the above copyright''), Document(metadata={''path'':
    ''.venv/lib/python3.12/site-packages/charset_normalizer-3.4.3.dist-info/entry_points.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 65}, page_content=''[console_scripts]\nnormalizer
    = charset_normalizer.cli:cli_detect''), Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/charset_normalizer-3.4.3.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 19}, page_content=''charset_normalizer''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/pyarrow-21.0.0.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 18}, page_content=''__dummy__\npyarrow''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/pytz-2025.2.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 5}, page_content=''pytz''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/pytz-2025.2.dist-info/LICENSE.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 1088}, page_content=''Copyright
    (c) 2003-2019 Stuart Bishop <stuart@stuartbishop.net>\n\nPermission is hereby
    granted, free of charge, to any person obtaining a\ncopy of this software and
    associated documentation files (the "Software"),\nto deal in the Software without
    restriction, including without limitation\nthe rights to use, copy, modify, merge,
    publish, distribute, sublicense,\nand/or sell copies of the Software, and to permit
    persons to whom the\nSoftware is furnished to do so, subject to the following
    conditions:\n\nThe above copyright notice and this permission notice shall be
    included in''), Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/geopandas-1.1.1.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 10}, page_content=''geopandas''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/google_api_python_client-2.182.0.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 58}, page_content=''apiclient\ngoogleapiclient\ngoogleapiclient/discovery_cache''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/pip-25.1.1.dist-info/entry_points.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 87}, page_content=''[console_scripts]\npip
    = pip._internal.cli.main:main\npip3 = pip._internal.cli.main:main''), Document(metadata={''path'':
    ''.venv/lib/python3.12/site-packages/pip-25.1.1.dist-info/top_level.txt'', ''type'':
    ''documentation'', ''directory'': ''root'', ''size_bytes'': 4}, page_content=''pip''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/httplib2/cacerts.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 137365},
    page_content=''# Issuer: CN=GTE CyberTrust Global Root O=GTE Corporation OU=GTE
    CyberTrust Solutions, Inc.\n# Subject: CN=GTE CyberTrust Global Root O=GTE Corporation
    OU=GTE CyberTrust Solutions, Inc.\n# Label: "GTE CyberTrust Global Root"\n# Serial:
    421\n# MD5 Fingerprint: ca:3d:d3:68:f1:03:5c:d0:32:fa:b8:2b:59:e8:5a:db\n# SHA1
    Fingerprint: 97:81:79:50:d8:1c:96:70:cc:34:d8:09:cf:79:44:31:36:7e:f4:74\n# SHA256
    Fingerprint: a5:31:25:18:8d:21:10:aa:96:4b:02:c7:b7:c6:da:32:03:17:08:94:e5:fb:71:ff:fb:66:67:d5:e6:81:0a:36\n-----BEGIN
    CERTIFICATE-----\nMIICWjCCAcMCAgGlMA0GCSqGSIb3DQEBBAUAMHUxCzAJBgNVBAYTAlVTMRgwFgYD\nVQQKEw9HVEUgQ29ycG9yYXRpb24xJzAlBgNVBAsTHkdURSBDeWJlclRydXN0IFNv''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/langchain_core-0.3.76.dist-info/entry_points.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 34}, page_content=''[console_scripts]\n\n[gui_scripts]\n''),
    Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/openpyxl-3.1.5.dist-info/LICENCE.rst'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 1131}, page_content=''This
    software is under the MIT Licence\n======================================\n\nCopyright
    (c) 2010 openpyxl\n\nPermission is hereby granted, free of charge, to any person
    obtaining a\ncopy of this software and associated documentation files (the\n"Software"),
    to deal in the Software without restriction, including\nwithout limitation the
    rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell
    copies of the Software, and to''), Document(metadata={''path'': ''.venv/lib/python3.12/site-packages/openpyxl-3.1.5.dist-info/top_level.txt'',
    ''type'': ''documentation'', ''directory'': ''root'', ''size_bytes'': 9}, page_content=''openpyxl'')]'
  data_models:
  - path: bird_demo/models.py
    type: data_model
    models_found:
    - BirdData
    preview: "import datetime\nimport zoneinfo\nfrom pydantic import BaseModel, field_validator\n\
      class BirdData(BaseModel):\n    def parse_date(cls, v: str) -> datetime.date:\n\
      # models.py\n\"\"\"Pydantic models for the example.\"\"\"\n\nimport datetime\n\
      import zoneinfo\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass\
      \ BirdData(BaseModel):"
  - path: .venv/lib/python3.12/site-packages/googleapiclient/model.py
    type: data_model
    models_found: []
    preview: 'from __future__ import absolute_import

      import json

      import logging

      import platform

      import urllib

      import warnings

      from googleapiclient import version as googleapiclient_version

      # Copyright 2014 Google Inc. All Rights Reserved.

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #      http://www.apache.org/licenses/LICENSE-2.0

      #'
  - path: .venv/lib/python3.12/site-packages/langsmith/schemas.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      from datetime import datetime, timedelta, timezone

      from decimal import Decimal

      from enum import Enum

      from typing import (

      from uuid import UUID

      from typing_extensions import NotRequired, TypedDict

      """Schemas for the LangSmith API."""


      from __future__ import annotations


      from datetime import datetime, timedelta, timezone

      from decimal import Decimal

      from enum import Enum

      from typing import ('
  - path: .venv/lib/python3.12/site-packages/mypy/types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import sys

      from abc import abstractmethod

      from collections.abc import Iterable, Sequence

      from typing import TYPE_CHECKING, Any, ClassVar, Final, NewType, TypeVar, Union,
      cast, overload

      from typing_extensions import Self, TypeAlias as _TypeAlias, TypeGuard

      import mypy.nodes

      """Classes for representing mypy types."""


      from __future__ import annotations


      import sys

      from abc import abstractmethod

      from collections.abc import Iterable, Sequence

      from typing import TYPE_CHECKING, Any, ClassVar, Final, NewType, TypeVar, Union,
      cast, overload'
  - path: .venv/lib/python3.12/site-packages/mypy/typeanal.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import itertools

      from collections.abc import Iterable, Iterator, Sequence

      from contextlib import contextmanager

      from typing import Callable, Final, Protocol, TypeVar

      from mypy import errorcodes as codes, message_registry, nodes

      from mypy.errorcodes import ErrorCode

      """Semantic analysis of types"""


      from __future__ import annotations


      import itertools

      from collections.abc import Iterable, Iterator, Sequence

      from contextlib import contextmanager

      from typing import Callable, Final, Protocol, TypeVar'
  - path: .venv/lib/python3.12/site-packages/mypy/subtypes.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      from collections.abc import Iterable, Iterator

      from contextlib import contextmanager

      from typing import Any, Callable, Final, TypeVar, cast

      from typing_extensions import TypeAlias as _TypeAlias

      import mypy.applytype

      import mypy.constraints

      from __future__ import annotations


      from collections.abc import Iterable, Iterator

      from contextlib import contextmanager

      from typing import Any, Callable, Final, TypeVar, cast

      from typing_extensions import TypeAlias as _TypeAlias


      import mypy.applytype'
  - path: .venv/lib/python3.12/site-packages/anthropic/_types.py
    type: data_model
    models_found: []
    preview: "from __future__ import annotations\nfrom os import PathLike\nfrom typing\
      \ import (\nfrom typing_extensions import (\nimport httpx\nimport pydantic\n\
      from httpx import URL, Proxy, Timeout, Response, BaseTransport, AsyncBaseTransport\n\
      from __future__ import annotations\n\nfrom os import PathLike\nfrom typing import\
      \ (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Dict,"
  - path: .venv/lib/python3.12/site-packages/anthropic/_models.py
    type: data_model
    models_found: []
    preview: "from __future__ import annotations\nimport os\nimport inspect\nfrom\
      \ typing import TYPE_CHECKING, Any, Type, Union, Generic, TypeVar, Callable,\
      \ Optional, cast\nfrom datetime import date, datetime\nfrom typing_extensions\
      \ import (\nimport pydantic\nfrom __future__ import annotations\n\nimport os\n\
      import inspect\nfrom typing import TYPE_CHECKING, Any, Type, Union, Generic,\
      \ TypeVar, Callable, Optional, cast\nfrom datetime import date, datetime\nfrom\
      \ typing_extensions import (\n    List,"
  - path: .venv/lib/python3.12/site-packages/pydantic/types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations as _annotations

      import base64

      import dataclasses as _dataclasses

      import re

      from collections.abc import Hashable, Iterator

      from datetime import date, datetime

      from decimal import Decimal

      """The types module contains custom types used by pydantic."""


      from __future__ import annotations as _annotations


      import base64

      import dataclasses as _dataclasses

      import re

      from collections.abc import Hashable, Iterator'
  - path: .venv/lib/python3.12/site-packages/pydantic/json_schema.py
    type: data_model
    models_found: []
    preview: "from __future__ import annotations as _annotations\nimport dataclasses\n\
      import inspect\nimport math\nimport os\nimport re\nimport warnings\n\"\"\"!!!\
      \ abstract \"Usage Documentation\"\n    [JSON Schema](../concepts/json_schema.md)\n\
      \nThe `json_schema` module contains classes and functions to allow the way [JSON\
      \ Schema](https://json-schema.org/)\nis generated to be customized.\n\nIn general\
      \ you shouldn't need to use this module directly; instead, you can use\n[`BaseModel.model_json_schema`][pydantic.BaseModel.model_json_schema]\
      \ and"
  - path: .venv/lib/python3.12/site-packages/pydantic/root_model.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations as _annotations

      import typing

      from copy import copy, deepcopy

      from pydantic_core import PydanticUndefined

      from . import PydanticUserError

      from ._internal import _model_construction, _repr

      from .main import BaseModel, _object_setattr

      """RootModel class and type definitions."""


      from __future__ import annotations as _annotations


      import typing

      from copy import copy, deepcopy


      from pydantic_core import PydanticUndefined'
  - path: .venv/lib/python3.12/site-packages/pydantic/type_adapter.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations as _annotations

      import sys

      from collections.abc import Callable, Iterable

      from dataclasses import is_dataclass

      from types import FrameType

      from typing import (

      from pydantic_core import CoreSchema, SchemaSerializer, SchemaValidator, Some

      """Type adapter specification."""


      from __future__ import annotations as _annotations


      import sys

      from collections.abc import Callable, Iterable

      from dataclasses import is_dataclass

      from types import FrameType'
  - path: .venv/lib/python3.12/site-packages/pydantic_core/core_schema.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations as _annotations

      import sys

      import warnings

      from collections.abc import Hashable, Mapping

      from datetime import date, datetime, time, timedelta

      from decimal import Decimal

      from re import Pattern

      """

      This module contains definitions to build schemas which `pydantic_core` can

      validate and serialize.

      """


      from __future__ import annotations as _annotations


      import sys'
  - path: .venv/lib/python3.12/site-packages/openai/_types.py
    type: data_model
    models_found: []
    preview: "from __future__ import annotations\nfrom os import PathLike\nfrom typing\
      \ import (\nfrom typing_extensions import (\nimport httpx\nimport pydantic\n\
      from httpx import URL, Proxy, Timeout, Response, BaseTransport, AsyncBaseTransport\n\
      from __future__ import annotations\n\nfrom os import PathLike\nfrom typing import\
      \ (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Dict,"
  - path: .venv/lib/python3.12/site-packages/openai/_models.py
    type: data_model
    models_found: []
    preview: "from __future__ import annotations\nimport os\nimport inspect\nfrom\
      \ typing import TYPE_CHECKING, Any, Type, Tuple, Union, Generic, TypeVar, Callable,\
      \ Optional, cast\nfrom datetime import date, datetime\nfrom typing_extensions\
      \ import (\nimport pydantic\nfrom __future__ import annotations\n\nimport os\n\
      import inspect\nfrom typing import TYPE_CHECKING, Any, Type, Tuple, Union, Generic,\
      \ TypeVar, Callable, Optional, cast\nfrom datetime import date, datetime\nfrom\
      \ typing_extensions import (\n    List,"
  - path: .venv/lib/python3.12/site-packages/_pytest/warning_types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import dataclasses

      import inspect

      from types import FunctionType

      from typing import Any

      from typing import final

      from typing import Generic

      from __future__ import annotations


      import dataclasses

      import inspect

      from types import FunctionType

      from typing import Any

      from typing import final

      from typing import Generic'
  - path: .venv/lib/python3.12/site-packages/google/auth/identity_pool.py
    type: data_model
    models_found: []
    preview: "    from collections.abc import Mapping\n    from collections import\
      \ Mapping  # type: ignore\nimport abc\nimport base64\nimport json\nimport os\n\
      from typing import NamedTuple\n# Copyright 2020 Google LLC\n#\n# Licensed under\
      \ the Apache License, Version 2.0 (the \"License\");\n# you may not use this\
      \ file except in compliance with the License.\n# You may obtain a copy of the\
      \ License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#"
  - path: .venv/lib/python3.12/site-packages/google/generativeai/models.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import typing

      from typing import Any, Literal

      import google.ai.generativelanguage as glm

      from google.generativeai import protos

      from google.generativeai import operations

      from google.generativeai.client import get_default_model_client

      # -*- coding: utf-8 -*-

      # Copyright 2023 Google LLC

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #     http://www.apache.org/licenses/LICENSE-2.0'
  - path: .venv/lib/python3.12/site-packages/google/oauth2/webauthn_types.py
    type: data_model
    models_found: []
    preview: "from dataclasses import dataclass\nimport json\nfrom typing import Any,\
      \ Dict, List, Optional\nfrom google.auth import exceptions\nclass PublicKeyCredentialDescriptor:\n\
      \    def to_dict(self):\nclass AuthenticationExtensionsClientInputs:\nfrom dataclasses\
      \ import dataclass\nimport json\nfrom typing import Any, Dict, List, Optional\n\
      \nfrom google.auth import exceptions\n\n\n@dataclass(frozen=True)"
  - path: .venv/lib/python3.12/site-packages/google/generativeai/types/retriever_types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import datetime

      import re

      import abc

      import dataclasses

      from typing import Any, AsyncIterable, Optional, Union, Iterable, Mapping

      from typing_extensions import deprecated  # type: ignore

      # -*- coding: utf-8 -*-

      # Copyright 2023 Google LLC

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #     http://www.apache.org/licenses/LICENSE-2.0'
  - path: .venv/lib/python3.12/site-packages/google/generativeai/types/permission_types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import dataclasses

      from typing import Optional, Union, Any, Iterable, AsyncIterable

      import re

      import google.ai.generativelanguage as glm

      from google.generativeai import protos

      from google.protobuf import field_mask_pb2

      # -*- coding: utf-8 -*-

      # Copyright 2024 Google LLC

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #     http://www.apache.org/licenses/LICENSE-2.0'
  - path: .venv/lib/python3.12/site-packages/google/generativeai/types/generation_types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import collections

      import contextlib

      from collections.abc import Iterable, AsyncIterable, Mapping

      import dataclasses

      import itertools

      import json

      # -*- coding: utf-8 -*-

      # Copyright 2023 Google LLC

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #     http://www.apache.org/licenses/LICENSE-2.0'
  - path: .venv/lib/python3.12/site-packages/google/generativeai/types/helper_types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import google.api_core.timeout

      import google.api_core.retry

      import collections

      import dataclasses

      from typing import Union

      from typing_extensions import TypedDict

      # -*- coding: utf-8 -*-

      # Copyright 2023 Google LLC

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #     http://www.apache.org/licenses/LICENSE-2.0'
  - path: .venv/lib/python3.12/site-packages/google/generativeai/types/text_types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import sys

      import abc

      import dataclasses

      from typing import Any, Dict, List

      from typing_extensions import TypedDict

      from google.generativeai import string_utils

      # -*- coding: utf-8 -*-

      # Copyright 2023 Google LLC

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #     http://www.apache.org/licenses/LICENSE-2.0'
  - path: .venv/lib/python3.12/site-packages/google/generativeai/types/model_types.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      from collections.abc import Mapping

      import csv

      import dataclasses

      import datetime

      import json

      import pathlib

      # -*- coding: utf-8 -*-

      # Copyright 2023 Google LLC

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #     http://www.apache.org/licenses/LICENSE-2.0'
  - path: .venv/lib/python3.12/site-packages/google/generativeai/notebook/lib/model.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import abc

      import dataclasses

      from typing import Sequence

      class ModelArguments:

      class ModelResults:

      class AbstractModel(abc.ABC):

      # -*- coding: utf-8 -*-

      # Copyright 2023 Google LLC

      #

      # Licensed under the Apache License, Version 2.0 (the "License");

      # you may not use this file except in compliance with the License.

      # You may obtain a copy of the License at

      #

      #     http://www.apache.org/licenses/LICENSE-2.0'
  - path: .venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import asyncio

      import inspect

      import json

      import typing

      import warnings

      from abc import ABC, abstractmethod

      """Chat models for conversational AI."""


      from __future__ import annotations


      import asyncio

      import inspect

      import json

      import typing'
  - path: .venv/lib/python3.12/site-packages/langchain_core/utils/pydantic.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import inspect

      import textwrap

      import warnings

      from contextlib import nullcontext

      from functools import lru_cache, wraps

      from types import GenericAlias

      """Utilities for pydantic."""


      from __future__ import annotations


      import inspect

      import textwrap

      import warnings

      from contextlib import nullcontext'
  - path: .venv/lib/python3.12/site-packages/langchain_core/output_parsers/pydantic.py
    type: data_model
    models_found: []
    preview: 'import json

      from typing import Annotated, Generic, Optional

      import pydantic

      from pydantic import SkipValidation

      from typing_extensions import override

      from langchain_core.exceptions import OutputParserException

      from langchain_core.output_parsers import JsonOutputParser

      """Output parsers using Pydantic."""


      import json

      from typing import Annotated, Generic, Optional


      import pydantic

      from pydantic import SkipValidation

      from typing_extensions import override'
  - path: .venv/lib/python3.12/site-packages/langchain_core/tracers/schemas.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import warnings

      from datetime import datetime, timezone

      from typing import Any, Optional

      from uuid import UUID

      from langsmith import RunTree

      from langsmith.schemas import RunTypeEnum as RunTypeEnumDep

      """Schemas for tracers."""


      from __future__ import annotations


      import warnings

      from datetime import datetime, timezone

      from typing import Any, Optional

      from uuid import UUID'
  - path: .venv/lib/python3.12/site-packages/anthropic/types/model_info.py
    type: data_model
    models_found: []
    preview: 'from datetime import datetime

      from typing_extensions import Literal

      from .._models import BaseModel

      class ModelInfo(BaseModel):

      # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for
      details.


      from datetime import datetime

      from typing_extensions import Literal


      from .._models import BaseModel


      __all__ = ["ModelInfo"]



      class ModelInfo(BaseModel):'
  - path: .venv/lib/python3.12/site-packages/anthropic/types/beta/beta_model_info.py
    type: data_model
    models_found: []
    preview: 'from datetime import datetime

      from typing_extensions import Literal

      from ..._models import BaseModel

      class BetaModelInfo(BaseModel):

      # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for
      details.


      from datetime import datetime

      from typing_extensions import Literal


      from ..._models import BaseModel


      __all__ = ["BetaModelInfo"]



      class BetaModelInfo(BaseModel):'
  - path: .venv/lib/python3.12/site-packages/anthropic/lib/streaming/_types.py
    type: data_model
    models_found: []
    preview: "from typing import Union\nfrom typing_extensions import List, Literal,\
      \ Annotated\nfrom ...types import (\nfrom ..._models import BaseModel\nfrom\
      \ ..._utils._transform import PropertyInfo\nfrom ...types.citations_delta import\
      \ Citation\nclass TextEvent(BaseModel):\nfrom typing import Union\nfrom typing_extensions\
      \ import List, Literal, Annotated\n\nfrom ...types import (\n    Message,\n\
      \    ContentBlock,\n    MessageDeltaEvent as RawMessageDeltaEvent,\n    MessageStartEvent\
      \ as RawMessageStartEvent,"
  - path: .venv/lib/python3.12/site-packages/anthropic/lib/streaming/_beta_types.py
    type: data_model
    models_found: []
    preview: "from typing import Union\nfrom typing_extensions import List, Literal,\
      \ Annotated\nfrom ..._models import BaseModel\nfrom ...types.beta import (\n\
      from ..._utils._transform import PropertyInfo\nfrom ...types.beta.beta_citations_delta\
      \ import Citation\nclass BetaTextEvent(BaseModel):\nfrom typing import Union\n\
      from typing_extensions import List, Literal, Annotated\n\nfrom ..._models import\
      \ BaseModel\nfrom ...types.beta import (\n    BetaMessage,\n    BetaContentBlock,\n\
      \    BetaRawMessageStopEvent,"
  - path: .venv/lib/python3.12/site-packages/pydantic/v1/types.py
    type: data_model
    models_found: []
    preview: 'import abc

      import math

      import re

      import warnings

      from datetime import date

      from decimal import Decimal, InvalidOperation

      from enum import Enum

      import abc

      import math

      import re

      import warnings

      from datetime import date

      from decimal import Decimal, InvalidOperation

      from enum import Enum

      from pathlib import Path'
  - path: .venv/lib/python3.12/site-packages/pydantic/v1/annotated_types.py
    type: data_model
    models_found: []
    preview: "import sys\nfrom typing import TYPE_CHECKING, Any, Dict, FrozenSet,\
      \ NamedTuple, Type\nfrom pydantic.v1.fields import Required\nfrom pydantic.v1.main\
      \ import BaseModel, create_model\nfrom pydantic.v1.typing import is_typeddict,\
      \ is_typeddict_special\n    from typing_extensions import TypedDict\n    def\
      \ is_legacy_typeddict(typeddict_cls: Type['TypedDict']) -> bool:  # type: ignore[valid-type]\n\
      import sys\nfrom typing import TYPE_CHECKING, Any, Dict, FrozenSet, NamedTuple,\
      \ Type\n\nfrom pydantic.v1.fields import Required\nfrom pydantic.v1.main import\
      \ BaseModel, create_model\nfrom pydantic.v1.typing import is_typeddict, is_typeddict_special\n\
      \nif TYPE_CHECKING:"
  - path: .venv/lib/python3.12/site-packages/pydantic/v1/schema.py
    type: data_model
    models_found: []
    preview: 'import re

      import warnings

      from collections import defaultdict

      from dataclasses import is_dataclass

      from datetime import date, datetime, time, timedelta

      from decimal import Decimal

      from enum import Enum

      import re

      import warnings

      from collections import defaultdict

      from dataclasses import is_dataclass

      from datetime import date, datetime, time, timedelta

      from decimal import Decimal

      from enum import Enum

      from ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address,
      IPv6Interface, IPv6Network'
  - path: .venv/lib/python3.12/site-packages/pydantic/_internal/_model_construction.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations as _annotations

      import builtins

      import operator

      import sys

      import typing

      import warnings

      import weakref

      """Private logic for creating models."""


      from __future__ import annotations as _annotations


      import builtins

      import operator

      import sys

      import typing'
  - path: .venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations as _annotations

      import collections.abc

      import dataclasses

      import datetime

      import inspect

      import os

      import pathlib

      """Convert python types to pydantic-core schema."""


      from __future__ import annotations as _annotations


      import collections.abc

      import dataclasses

      import datetime

      import inspect'
  - path: .venv/lib/python3.12/site-packages/pydantic/_internal/_schema_gather.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      from dataclasses import dataclass, field

      from typing import TypedDict

      from pydantic_core.core_schema import ComputedField, CoreSchema, DefinitionReferenceSchema,
      SerSchema

      from typing_extensions import TypeAlias

      class GatherResult(TypedDict):

      class MissingDefinitionError(LookupError):

      # pyright: reportTypedDictNotRequiredAccess=false, reportGeneralTypeIssues=false,
      reportArgumentType=false, reportAttributeAccessIssue=false

      from __future__ import annotations


      from dataclasses import dataclass, field

      from typing import TypedDict


      from pydantic_core.core_schema import ComputedField, CoreSchema, DefinitionReferenceSchema,
      SerSchema

      from typing_extensions import TypeAlias'
  - path: .venv/lib/python3.12/site-packages/openai/types/model_deleted.py
    type: data_model
    models_found: []
    preview: "from .._models import BaseModel\nclass ModelDeleted(BaseModel):\n# File\
      \ generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\
      \nfrom .._models import BaseModel\n\n__all__ = [\"ModelDeleted\"]\n\n\nclass\
      \ ModelDeleted(BaseModel):\n    id: str\n\n    deleted: bool\n\n    object:\
      \ str"
  - path: .venv/lib/python3.12/site-packages/openai/types/model.py
    type: data_model
    models_found: []
    preview: "from typing_extensions import Literal\nfrom .._models import BaseModel\n\
      class Model(BaseModel):\n# File generated from our OpenAPI spec by Stainless.\
      \ See CONTRIBUTING.md for details.\n\nfrom typing_extensions import Literal\n\
      \nfrom .._models import BaseModel\n\n__all__ = [\"Model\"]\n\n\nclass Model(BaseModel):\n\
      \    id: str\n    \"\"\"The model identifier, which can be referenced in the\
      \ API endpoints.\"\"\""
  - path: .venv/lib/python3.12/site-packages/openai/cli/_models.py
    type: data_model
    models_found: []
    preview: "from typing import Any\nfrom typing_extensions import ClassVar\nimport\
      \ pydantic\nfrom .. import _models\nfrom .._compat import PYDANTIC_V1, ConfigDict\n\
      class BaseModel(_models.BaseModel):\n        class Config(pydantic.BaseConfig):\
      \  # type: ignore\nfrom typing import Any\nfrom typing_extensions import ClassVar\n\
      \nimport pydantic\n\nfrom .. import _models\nfrom .._compat import PYDANTIC_V1,\
      \ ConfigDict\n"
  - path: .venv/lib/python3.12/site-packages/openai/lib/_pydantic.py
    type: data_model
    models_found: []
    preview: 'from __future__ import annotations

      import inspect

      from typing import Any, TypeVar

      from typing_extensions import TypeGuard

      import pydantic

      from .._types import NOT_GIVEN

      from .._utils import is_dict as _is_dict, is_list

      from __future__ import annotations


      import inspect

      from typing import Any, TypeVar

      from typing_extensions import TypeGuard


      import pydantic

      '
  - path: .venv/lib/python3.12/site-packages/openai/types/responses/tool_choice_types.py
    type: data_model
    models_found: []
    preview: "from typing_extensions import Literal\nfrom ..._models import BaseModel\n\
      class ToolChoiceTypes(BaseModel):\n# File generated from our OpenAPI spec by\
      \ Stainless. See CONTRIBUTING.md for details.\n\nfrom typing_extensions import\
      \ Literal\n\nfrom ..._models import BaseModel\n\n__all__ = [\"ToolChoiceTypes\"\
      ]\n\n\nclass ToolChoiceTypes(BaseModel):\n    type: Literal[\n        \"file_search\"\
      ,"
  - path: .venv/lib/python3.12/site-packages/openai/types/responses/response_format_text_json_schema_config.py
    type: data_model
    models_found: []
    preview: 'from typing import Dict, Optional

      from typing_extensions import Literal

      from pydantic import Field as FieldInfo

      from ..._models import BaseModel

      class ResponseFormatTextJSONSchemaConfig(BaseModel):

      # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for
      details.


      from typing import Dict, Optional

      from typing_extensions import Literal


      from pydantic import Field as FieldInfo


      from ..._models import BaseModel


      __all__ = ["ResponseFormatTextJSONSchemaConfig"]'
  - path: .venv/lib/python3.12/site-packages/openai/types/shared/response_format_json_schema.py
    type: data_model
    models_found: []
    preview: 'from typing import Dict, Optional

      from typing_extensions import Literal

      from pydantic import Field as FieldInfo

      from ..._models import BaseModel

      class JSONSchema(BaseModel):

      class ResponseFormatJSONSchema(BaseModel):

      # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for
      details.


      from typing import Dict, Optional

      from typing_extensions import Literal


      from pydantic import Field as FieldInfo


      from ..._models import BaseModel

      '
  - path: .venv/lib/python3.12/site-packages/openai/types/graders/score_model_grader.py
    type: data_model
    models_found: []
    preview: 'from typing import List, Union, Optional

      from typing_extensions import Literal, TypeAlias

      from ..._models import BaseModel

      from ..shared.reasoning_effort import ReasoningEffort

      from ..responses.response_input_text import ResponseInputText

      from ..responses.response_input_audio import ResponseInputAudio

      class InputContentOutputText(BaseModel):

      # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for
      details.


      from typing import List, Union, Optional

      from typing_extensions import Literal, TypeAlias


      from ..._models import BaseModel

      from ..shared.reasoning_effort import ReasoningEffort

      from ..responses.response_input_text import ResponseInputText'
  - path: .venv/lib/python3.12/site-packages/openai/types/graders/label_model_grader.py
    type: data_model
    models_found: []
    preview: 'from typing import List, Union, Optional

      from typing_extensions import Literal, TypeAlias

      from ..._models import BaseModel

      from ..responses.response_input_text import ResponseInputText

      from ..responses.response_input_audio import ResponseInputAudio

      class InputContentOutputText(BaseModel):

      class InputContentInputImage(BaseModel):

      # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for
      details.


      from typing import List, Union, Optional

      from typing_extensions import Literal, TypeAlias


      from ..._models import BaseModel

      from ..responses.response_input_text import ResponseInputText

      from ..responses.response_input_audio import ResponseInputAudio'
  - path: .venv/lib/python3.12/site-packages/openai/cli/_api/models.py
    type: data_model
    models_found: []
    preview: "from __future__ import annotations\nfrom typing import TYPE_CHECKING\n\
      from argparse import ArgumentParser\nfrom .._utils import get_client, print_model\n\
      from .._models import BaseModel\n    from argparse import _SubParsersAction\n\
      def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\nfrom __future__\
      \ import annotations\n\nfrom typing import TYPE_CHECKING\nfrom argparse import\
      \ ArgumentParser\n\nfrom .._utils import get_client, print_model\nfrom .._models\
      \ import BaseModel\n"
  - path: metacontext/schemas/extensions/models.py
    type: data_model
    models_found: []
    preview: "from typing import Any\nfrom pydantic import BaseModel, Field, model_validator\n\
      from metacontext.schemas.extensions.base import (\nclass ModelDeterministicMetadata(DeterministicMetadata):\n\
      class TrainingData(BaseModel):\n    def clean_feature_descriptions(cls, values:\
      \ dict[str, Any]) -> dict[str, Any]:\nclass ModelAIEnrichment(AIEnrichment):\n\
      \"\"\"Model extension schemas.\"\"\"\n\nfrom typing import Any\n\nfrom pydantic\
      \ import BaseModel, Field, model_validator\n\nfrom metacontext.schemas.extensions.base\
      \ import (\n    AIEnrichment,"
  - path: metacontext/ai/prompts/schema_utils.py
    type: data_model
    models_found: []
    preview: 'import json

      import string

      from typing import Any

      from pydantic import BaseModel

      def generate_field_descriptions(model_class: type[BaseModel]) -> str:

      def generate_json_schema(model_class: type[BaseModel]) -> str:

      def generate_prompt_from_schema(

      """Utilities for generating prompts from Pydantic schemas."""


      import json

      import string

      from typing import Any


      from pydantic import BaseModel

      '
  config_files:
  - path: Makefile
    type: configuration
    size_bytes: 175
    preview: ".PHONY: demo\n\ninstall: ## Install dependencies with Poetry\n\t@echo\
      \ \"Installing dependencies...\"\n\tpoetry install\n\ndemo: ## Run the bird_demo\n\
      \tpoetry run python bird_demo/demo.py"
  - path: pyproject.toml
    type: configuration
    size_bytes: 803
    preview: '[tool.poetry]

      name = "bird_demo"

      version = "0.1.0"

      description = "A project that uses birds to demonstrate how metacontext is used"

      authors = ["raymondpeterdorn <rpd346@gmail.com>"]

      readme = "README.md"

      packages = [{include = "bird_demo"}]


      [tool.poetry.dependencies]

      python = ">=3.12,<3.13"'
  - path: README.md
    type: configuration
    size_bytes: 2215
    preview: 'Bird Anatomy and Species Classification Project

      This repository contains an example codebase for data analysis and machine learning.
      The primary goal of this project is to explore the relationships between various
      anatomical measurements of birds and to build a predictive model for species
      classification.


      Business Context & Purpose

      The data was collected by a team of ornithologists to better understand the
      physical characteristics that define different bird species. The key questions
      we aim to answer are:


      What are the key statistical characteristics of the collected data?


      Can we predict a bird''s species based on its physical measurements alone?

      '
  - path: .venv/pyvenv.cfg
    type: configuration
    size_bytes: 507
    preview: 'home = /opt/homebrew/Cellar/python@3.12/3.12.11/bin

      implementation = CPython

      version_info = 3.12.11.final.0

      virtualenv = 20.32.0

      include-system-site-packages = false

      base-prefix = /opt/homebrew/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12

      base-exec-prefix = /opt/homebrew/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12

      base-executable = /opt/homebrew/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12/bin/python3.12

      prompt = bird-demo-py3.12'
  - path: .vscode/settings.json
    type: configuration
    size_bytes: 193
    preview: "{\n    \"python-envs.pythonProjects\": [\n        {\n            \"\
      path\": \"\",\n            \"envManager\": \"ms-python.python:venv\",\n    \
      \        \"packageManager\": \"ms-python.python:pip\"\n        }\n    ]\n}"
  - path: bird_demo/output/filtered_locations_metacontext.yaml
    type: configuration
    size_bytes: 598
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-13T23:52:01.847269+00:00'\n\
      \  generation_method: fallback\n  function_call: metacontext.metacontextualize()\n\
      file_info:\n  filename: filtered_locations.gpkg\n  extension: .gpkg\n  absolute_path:\
      \ /Users/bigdawg/Documents/metacontext/example/output/filtered_locations.gpkg\n\
      \  directory: example/output"
  - path: bird_demo/output/csv_metacontext.yaml
    type: configuration
    size_bytes: 562
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-14T01:18:13.856332+00:00'\n\
      \  generation_method: fallback\n  function_call: metacontext.metacontextualize()\n\
      file_info:\n  filename: csv.csv\n  extension: .csv\n  absolute_path: /Users/bigdawg/Documents/metacontext/example/output/csv.csv\n\
      \  directory: example/output"
  - path: bird_demo/output/pixel_bird_metacontext.yaml
    type: configuration
    size_bytes: 572
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-13T23:52:01.881455+00:00'\n\
      \  generation_method: fallback\n  function_call: metacontext.metacontextualize()\n\
      file_info:\n  filename: pixel_bird.png\n  extension: .png\n  absolute_path:\
      \ /Users/bigdawg/Documents/metacontext/example/output/pixel_bird.png\n  directory:\
      \ example/output"
  - path: bird_demo/output/xlsx_metacontext.yaml
    type: configuration
    size_bytes: 567
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-14T01:18:53.935251+00:00'\n\
      \  generation_method: fallback\n  function_call: metacontext.metacontextualize()\n\
      file_info:\n  filename: xlsx.xlsx\n  extension: .xlsx\n  absolute_path: /Users/bigdawg/Documents/metacontext/example/output/xlsx.xlsx\n\
      \  directory: example/output"
  - path: bird_demo/output/bird_classification_model_metacontext.yaml
    type: configuration
    size_bytes: 619
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-13T23:51:28.375188+00:00'\n\
      \  generation_method: fallback\n  function_call: metacontext.metacontextualize()\n\
      file_info:\n  filename: bird_classification_model.pkl\n  extension: .pkl\n \
      \ absolute_path: /Users/bigdawg/Documents/metacontext/example/output/bird_classification_model.pkl\n\
      \  directory: example/output"
  - path: bird_demo/output/processed_bird_data_metacontext.yaml
    type: configuration
    size_bytes: 3551
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-13T14:20:53.700859Z'\n\
      \  generation_method: explicit_function\n  function_call: metacontext.metacontextualize()\n\
      \  token_usage:\n    total_tokens: 0\n    prompt_tokens: 0\n    completion_tokens:\
      \ 0\n    total_api_calls: 1"
  - path: bird_demo/output/buffered_locations_metacontext.yaml
    type: configuration
    size_bytes: 607
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-13T23:51:44.926149+00:00'\n\
      \  generation_method: fallback\n  function_call: metacontext.metacontextualize()\n\
      file_info:\n  filename: buffered_locations.geojson\n  extension: .geojson\n\
      \  absolute_path: /Users/bigdawg/Documents/metacontext/example/output/buffered_locations.geojson\n\
      \  directory: example/output"
  - path: bird_demo/output/eda_csv_metacontext.yaml
    type: configuration
    size_bytes: 570
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-14T01:18:38.185985+00:00'\n\
      \  generation_method: fallback\n  function_call: metacontext.metacontextualize()\n\
      file_info:\n  filename: eda_csv.csv\n  extension: .csv\n  absolute_path: /Users/bigdawg/Documents/metacontext/example/output/eda_csv.csv\n\
      \  directory: example/output"
  - path: bird_demo/data/birdos.metacontext.json
    type: configuration
    size_bytes: 182285
    preview: "{\n  \"metacontext_version\": \"0.3.0\",\n  \"generation_info\": {\n\
      \    \"generated_at\": \"2025-09-14T02:03:01.992494+00:00\",\n    \"generation_method\"\
      : \"fallback\",\n    \"function_call\": \"metacontext.metacontextualize()\"\n\
      \  },\n  \"file_info\": {\n    \"filename\": \"birdos.csv\",\n    \"extension\"\
      : \".csv\","
  - path: bird_demo/data/birdos_metacontext.yaml
    type: configuration
    size_bytes: 170913
    preview: "metacontext_version: 0.3.0\ngeneration_info:\n  generated_at: '2025-09-14T02:06:53.676205+00:00'\n\
      \  generation_method: fallback\n  function_call: metacontext.metacontextualize()\n\
      file_info:\n  filename: birdos.csv\n  extension: .csv\n  absolute_path: /Users/bigdawg/Documents/metacontext/example/data/birdos.csv\n\
      \  directory: example/data"
  cross_references:
    referenced_by:
    - file: bird_demo/demo.py
      file_type: .py
      references:
      - line_number: 303
        line_content: img_path = Path("example/output/pixel_bird.png")
        match: '"example/output/pixel_bird.png"'
        reference_type: path_reference
        context:
        - ''
        - '# Save the image'
        - img_path = Path("example/output/pixel_bird.png")
        - bird_img.save(img_path)
        - ''
      - line_number: 303
        line_content: img_path = Path("example/output/pixel_bird.png")
        match: Path("example/output/pixel_bird.png"
        reference_type: data_loading
        context:
        - ''
        - '# Save the image'
        - img_path = Path("example/output/pixel_bird.png")
        - bird_img.save(img_path)
        - ''
      reference_count: 2
    imports_from: []
    data_dependencies: []
    summary: Referenced by 1 file(s)
  scan_summary:
    total_files_scanned: 18193
    related_code_files: 10
    documentation_files: 20
    data_model_files: 52
    config_files: 15
    cross_reference_files: 1
    scan_depth: 12
